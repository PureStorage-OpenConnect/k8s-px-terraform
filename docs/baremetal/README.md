# Terraform-iac

Terraform-iac is a terraform library that manages the lifecycle of EKS/AKS/GKE clusters for Purestorage Portworx capability demo environments

PreRequisites:
1. AWSCLI/GCloud/Azure CLI installed and Keys are created with "Admin" Role (Please review [References](#references) section if you need to install)
2. GIT (to download the GIT repository that contains terraform code)
3. Terraform (basic troubleshooting knowledge)
4. Portworx is setup and has a valid license
5. KubeCtl package needs to be installed
6. Access to terraform-iac repository. Reach out PureStorage team for the same,
7. These steps will work with Linux or MAC OS. For Windows please use Docker image.

## Setup Terraform

You can use terraform in two ways:

#### A. Local Terraform

#### B. Using Docker image  (Optional)
--------------------------------------------------------------------------------------------------------------
## A. Local Terraform
### 1. Install Terraform:
Install Terraform if its already not installed:
Note: the Terraform version should be (version 1.0.11)

```
wget -q -O/tmp/terraform.zip https://releases.hashicorp.com/terraform/1.0.11/terraform_1.0.11_linux_amd64.zip 
unzip -q -d /usr/bin /tmp/terraform.zip 
rm /tmp/terraform.zip

```

### 2. Get the Terraform code from the repository

Download the latest source from [git](https://github.com/PureStorage-OpenConnect/k8s-px-terraform.git/) to have latest terraform-iac library. Alternatively, git pull command will bring the latest code if you already have the source code pulled

```
git clone https://github.com/PureStorage-OpenConnect/k8s-px-terraform.git.git
```

### 3. Setup Cloud Env Profile

#### AWS 

The AWS CLI stores sensitive credential information that you specify with `aws configure` in a local file named credentials, in a folder named .aws in your home directory.


For example, the files generated by the AWS CLI for a default profile configured with aws configure looks similar to the following.

~/.aws/credentials

```
[default]
aws_access_key_id=AKIAIOSFODNN7EXAMPLE
aws_secret_access_key=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY

[purestorage-dev]
aws_access_key_id=ALIATHISODNN7EXAMPLE
aws_secret_access_key=wJalrXUtnFEMI/K7EXXY/bPxRfiCYEXAMPLEKEY

```

Export AWS profile by pointing to the desired account.

```
export AWS_PROFILE=<purestorage_dev> -- Please ensure that this entry exists in the ~/.aws/credentials file

Test connectivity: 
aws s3 ls

```

In case you want setup new AWS profile, more information can be found [here.](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-profiles.html)

You will need access_key and secret_key of your AWS account

This profile is going to be used for creating the EKS Cluster

NOTE: This same keys are also used to provision portworx storage as well


#### Google Cloud:

```
Create a new Service Account if you do not have one provided (and export JSON steps) :
Navigate to Google cloud console for your org (https://console.cloud.google.com/) - Login with your credentials
Navigate to IAM & Admin 
Click on Service Accounts
    Create new Service Account if you dont have one
    Choose Name (for ex: svcPortworxDemo )
        Permissions: 
          1. Compute Admin
          2. Service Account User
          3. Kubernetes Engine Admin
          4. Project IAM Admin
          5. Service Account Token Creator
        Keys:
            Generate a new Key and export the JSON file (this will be used in terraform.tfvars)
    Ensure you are connecting to the service account
        1. gcloud auth revoke <Revoke all active authentications>
        2. gcloud init (choose your servive-account, project-id, region, zone etc)
        2. gcloud auth activate-service-account --key-file=/Users/t_gadar/Downloads/svcPortworxDemo-credentials-52bc683c4a93.json
    
```

#### Azure Cloud:
```
Follow the document created at ./docs/AzureSteps.md file for setting up a new AZ account with "Service Principle"

```

### 4. Navigate to scripts folder and Run setup_env.sh <param1> <param2> <param3>

```
./setup_env.sh <Provider> <UniqueIdForCluster> <ZoneName>

./setup_env.sh help

~/PureStorage-OpenConnect/k8s-px-terraform/scripts [master*] :./setup_env.sh help                                                           *[master][ruby-2.4.0]

Wed Feb  9 17:03:21 PST 2022 --> Usage: ./setup_env.sh [option...] {aws|gcloud|azure|vm|baremetal} {UniqueIdForTheCluster} {RegionOrDataCenter}
Wed Feb  9 17:03:21 PST 2022 -->  AWS    for example: ./setup_env.sh aws 1234567 us-west-2
Wed Feb  9 17:03:21 PST 2022 -->         for example: ./setup_env.sh aws 1234567 global [Note: requires Admin privileges to create Group, policy etc]
Wed Feb  9 17:03:21 PST 2022 -->  GCloud for example: ./setup_env.sh gcloud dev us-east4
Wed Feb  9 17:03:21 PST 2022 -->  Azure  for example: ./setup_env.sh azure dev us-east4
Wed Feb  9 17:03:21 PST 2022 -->  VM     for example: ./setup_env.sh vm cluster01 ps-lab-01


>Provider : aws, gcloud or zure.
>UniqueIdForCluster : It can be account number or project name.
>Zone : Zone in which cluster will be deployed.


 ~/PureStorage-OpenConnect/k8s-px-terraform/scripts (master) ⚡ :pwd
/Users/t_gadar/PureStorage-OpenConnect/k8s-px-terraform/scripts
 
 aws example:
 ~/PureStorage-OpenConnect/k8s-px-terraform/scripts (master) ⚡ :./setup_env.sh aws 896853494200 us-west-2
 
 gcloud example:
 ~/PureStorage-OpenConnect/k8s-px-terraform/scripts (master) ⚡ :./setup_env.sh gcloud PSGcloudDev us-central1
 
 azure aks example:
 ~/PureStorage-OpenConnect/k8s-px-terraform/scripts (master) ⚡ :./setup_env.sh azure PSAzureDev us-west3
 
 VM example:
 ~/PureStorage-OpenConnect/k8s-px-terraform/scripts (master) ⚡ :./setup_env.sh vm Dev PaloAltoDC

For VM and BareMetal, please follow the link for next Steps.. 
<LINK Here>
 
 
```

You will be be navigated to a new bash shell at the targeted location with the following files

The following entries are example, you should be seeing similar files and folder structure relative to where you have installed the terraform-iac git repo

#### AWS with "REGION" Example:
```
bash-3.2$ pwd
/Users/t_gadar/PureStorage-OpenConnect/k8s-px-terraform/terraform-live/aws/896853494200/us-west-2
bash-3.2$


/Users/t_gadar/PureStorage-OpenConnect/k8s-px-terraform/terraform-live/aws/896853494200/us-west-2
bash-3.2$ ls -alrt
total 64
drwxr-xr-x   3 t_gadar  staff    96 Jan  3 19:48 ..
drwxr-xr-x   4 t_gadar  staff   128 Jan  3 19:48 .terraform
-rw-r--r--   1 t_gadar  staff  3528 Jan  3 19:54 eks.tf
-rw-r--r--   1 t_gadar  staff  1123 Jan  3 19:54 network.tf
-rw-r--r--   1 t_gadar  staff    44 Jan  3 19:54 output.tf
-rw-r--r--   1 t_gadar  staff   512 Jan  3 19:54 portworx.tf
-rw-r--r--   1 t_gadar  staff  1237 Jan  3 19:54 provider.tf
-rw-r--r--   1 t_gadar  staff  1226 Jan  3 19:54 sg.tf
-rw-r--r--   1 t_gadar  staff   846 Jan  3 19:54 variable.tf
-rw-r--r--   1 t_gadar  staff   142 Jan  3 19:54 terraform.tfvars
drwxr-xr-x  11 t_gadar  staff   352 Jan  3 19:54 .
```

#### AWS with "global" Example:
```
 ~/PureStorage-OpenConnect/k8s-px-terraform/scripts [master*] :./setup_env.sh aws 12345 global                                               *[master][ruby-2.4.0]

Wed Feb  9 17:05:47 PST 2022 - Environment chosen is:    aws
Wed Feb  9 17:05:47 PST 2022 - Cluster Identifier:       12345
Wed Feb  9 17:05:47 PST 2022 - Region/Data Center:       global

Wed Feb  9 17:05:47 PST 2022 - Copied Files from the template folder to target location ../terraform-live/aws/12345/global -- completed
Changing directory to target directory - ../terraform-live/aws/12345/global
Opening a new shell with the Target directory

 ~/PureStorage-OpenConnect/k8s-px-terraform/terraform-live/aws/12345/global git:(master*) :ls -alrt *.tf
-rw-r--r--  1 t_gadar  staff  5206 Feb  9 17:05 policy.tf
-rw-r--r--  1 t_gadar  staff    45 Feb  9 17:05 provider.tf
 ~/PureStorage-OpenConnect/k8s-px-terraform/terraform-live/aws/12345/global git:(master*) :
```


#### Azure Example:
```
 ~/PureStorage-OpenConnect/k8s-px-terraform/terraform-live/azure/rgdev/us-west3 git:(master*) :ll
-rw-r--r--  1 t_gadar  staff    447 Jan 24 13:58 main.tf
-rw-r--r--  1 t_gadar  staff   1201 Jan 24 13:58 variable.tf
-rw-r--r--  1 t_gadar  staff   1229 Jan 24 13:58 terraform.tfvars
-rw-r--r--  1 t_gadar  staff    398 Jan 24 22:11 provider.tf
```

#### GCloud Example:
```
~/terraform-iac/terraform-live/gcloud/PSGcloudDev/us-central1 git/master*
 ~/PureStorage-OpenConnect/k8s-px-terraform/terraform-live/gcloud/PSGcloudDev/us-central1 git:(master*) :ls -alrt
drwxr-xr-x  3 t_gadar  staff    96 Jan 12 23:18 ..
-rw-r--r--  1 t_gadar  staff   571 Jan 12 23:21 terraform.tfvars
-rw-r--r--  1 t_gadar  staff  2244 Jan 24 23:01 gke.tf
-rw-r--r--  1 t_gadar  staff   153 Jan 24 23:01 provider.tf
-rw-r--r--  1 t_gadar  staff  1244 Jan 24 23:01 variable.tf
-rw-r--r--  1 t_gadar  staff   158 Jan 24 23:01 versions.tf
-rw-r--r--  1 t_gadar  staff   444 Jan 24 23:01 vpc.tf
drwxr-xr-x  8 t_gadar  staff   256 Jan 24 23:01 .

```


### 5. Configure terraform.tfvars [parameters]

Edit the file - nano terraform.tfvars and replace the following content and correct the values per the account

Ensure the key-pair has been created in the region prior to updating the terraform.tfvars file below:

In case you need to create the key pair, follow the steps:
- Login to AWS Console
- Navigate to EC2 Service
- Select/Click on "Key Pairs" on the left panel in Network & Security Section
- Click on "Create Key Pair" button and follow the steps
- Save the PEM key file that is downloaded automatically for future use to login to the nodes

For additional information, you can check the following link here https://docs.aws.amazon.com/cli/latest/userguide/cli-services-ec2-keypairs.html


### 6. Execute

```
terraform init
terraform validate
terraform plan -var-file="terraform.tfvars" -out plan.out
terraform apply "plan.out"
```

This completes the creation of EKS cluster with Portworx, and the output of cluster name is generated.
Note: A new kube config file will be created at ~/.kube/config, and the existing kube config file will be backed up with date and time stamp.


## Cleanup steps:

Step 1: Delete all namespaces that are created on the cluster using a script.

Step 2: Export desired AWS account profile (Skip if already exported) and then run the terraform destroy command:

```
export AWS_PROFILE=purestorage_aws_dev
terraform destroy -auto-approve
```

Note: the terraform destroy command needs to be executed from the same location as terraform apply command. 

## B. Using Docker image (Optional)


Build Docker Image using the below shell script;

```
./buildDockerImage.sh
```


Setup AWS credentials using the following environment variables:
```
export AWS_ACCESS_KEY_ID=$(aws --profile <aws_profile> configure get aws_access_key_id)
export AWS_SECRET_ACCESS_KEY=$(aws --profile <aws_profile> configure get aws_secret_access_key)
```

Execute the docker image using the below shell script:

``` 
./runDockerImage.sh
```

### Terraform Plan and Apply

Navigate to the folder that contains terraform files:

```
cd aws-<accountnumber>/<region>

The folder should have the following files:

~/PureStorage-OpenConnect/k8s-px-terraform/terraform-live/aws-077119361744/us-west-1 master  :ll
total 80
-rw-r--r--  1 t_gadar  staff   235 Dec 15 11:57 main.tf
-rw-r--r--  1 t_gadar  staff  1123 Dec 15 11:57 network.tf
-rw-r--r--  1 t_gadar  staff    44 Dec 15 11:57 output.tf
-rw-r--r--  1 t_gadar  staff   359 Dec 15 11:57 portworx.tf
-rw-r--r--  1 t_gadar  staff  1237 Dec 15 11:57 provider.tf
-rw-r--r--  1 t_gadar  staff  1226 Dec 15 11:57 sg.tf
-rw-r--r--  1 t_gadar  staff   509 Dec 15 12:00 variable.tf
-rw-r--r--  1 t_gadar  staff  3467 Dec 15 12:03 eks.tf

```

Follow the Step 3 - 6 from non-docker steps


## AWS IAM Permissions

Users who want to create EKS cluster with portworx would require permissions that are defined in the ./templates/aws/global

The group, policies can be created by running the following commands by an Admin

***** NOTE: Upon creating the group, users needs to be assigned to this newly created group *****

```


./setup_env.sh aws 1234567 global

terraform init
terraform validate
terraform plan -out "plan.out"
terraform apply "plan.out"

```

If you want to create manually:

Login to AWS console, Navigate to IAM 

Step 1: Create a custom policy that has limited IAM access and additional EKS custom permissions, the policy json is located in terraform-iac/scripts/eks-custom-iam.json

Step 2: Add the following AWS managed Policies to the user/group
```
AmazonEC2FullAccess
AmazonEKSClusterPolicy
AmazonEKSWorkerNodePolicy
AmazonS3ReadOnlyAccess
AmazonEKSServicePolicy
AmazonEKS_CNI_Policy
AmazonEKSFargatePodExecutionRolePolicy
AmazonEKSVPCResourceController
```

## References:

Install AWSCLI Guide - https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html

Setup AWS Keypair - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html#having-ec2-create-your-key-pair

Install GCloud SDK Guide - https://cloud.google.com/sdk/docs/quickstart

Install Azure Cli Guide - https://docs.microsoft.com/en-us/cli/azure/install-azure-cli-macos

Install and Configure GIT - https://git-scm.com/book/en/v2/Getting-Started-Installing-Git



## Contributing
Pull requests are welcome. For major changes, please open an issue first to discuss what you would like to change.

Please make sure to update tests as appropriate.



