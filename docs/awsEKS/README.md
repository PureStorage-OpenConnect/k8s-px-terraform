# Step by Step Guide to install AWS EKS with Portworx

## PreRequisite:
1. Please see docs/awsIAMPolicy/README.md for instruction on provisioning the IAM policies required for creating the cluster
   
2. Ensure the IAM with the required permissions has been provisioned by the admin
   
3. Have the role added to the user who is about to create the cluster


### Step 1. Install required software(s)

This repo contains scripts/prereq.sh file that will install all the required softwares based on the OS (tested on MacOS and Ubuntu)

Upon running the script the following software/tools will be installed that is required to create EKS cluster

1. Terraform
2. GIT
3. Docker
4. AWScli
5. Kubectl

For additional details and instructions on above installing above softwares are defined at [readme.md](../../README.md)


### Step 2. Download the IaC code

Download the latest source from [git](https://github.com/PureStorage-OpenConnect/k8s-px-terraform.git) to have latest terraform-iac library

If you already have the repo downloaded, git pull command will bring the latest code from the GIT master

```
    git clone https://github.com/PureStorage-OpenConnect/k8s-px-terraform.git

```

### Step 3. Setup AWS Profile

The AWS CLI stores sensitive credential information that you specify with `aws configure` in a local file named credentials, in a folder named .aws in your home directory.


For example, the files generated by the AWS CLI configure command for a default profile will look like the following

cat ~/.aws/credentials

```
[default]
aws_access_key_id=AKIAIOSFODNN7EXAMPLE
aws_secret_access_key=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY

[purestorage-dev]
aws_access_key_id=ALIATHISODNN7EXAMPLE
aws_secret_access_key=wJalrXUtnFEMI/K7EXXY/bPxRfiCYEXAMPLEKEY

```

Export AWS profile by pointing to the desired account

```
export AWS_PROFILE=<purestorage_profile> -- Please ensure that this entry exists in the ~/.aws/credentials file

Test connectivity: 
aws s3 ls
```

In case you want to setup new AWS profile, more information can be found [here.](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-profiles.html)

You will need the access_key and secret_key of your AWS account
NOTE: The same keys are also used to provision portworx storage as well


### Step 4. Navigate to scripts folder and Run setup_env.sh <param1> <param2> <param3>

```
./setup_env.sh <Provider> <UniqueIdForCluster> <ZoneName>

The three parameters:
1. Provider : aws, gcloud, azure, vm, baremetal
2. UniqueIdentifierForCluster : It can be account number along with unqiue ID.
3. Zone : AWS availability zone - in which cluster will be provisioned

 aws example:
 ~/purestorage/terraform-iac/scripts (master) âš¡ :./setup_env.sh aws 896853494200.bikas us-west-2
  
```

You will be be navigated to a new bash shell with all the required files copied and prepopulated

The following entries in the terraform.tfvars are example only, Please modify accoding to your needs 

You should be seeing similar files and folder structure relative to where you have installed the terraform-iac git repo

```
bash-3.2$ pwd
/PureStorage-OpenConnect/k8s-px-terraform/terraform-live/aws/896853494200/us-west-2
bash-3.2$


/PureStorage-OpenConnect/k8s-px-terraform/terraform-live/aws/896853494200/us-west-2
bash-3.2$ ls -alrt
total 64
drwxr-xr-x   3 t_gadar  staff    96 Jan  3 19:48 ..
drwxr-xr-x   4 t_gadar  staff   128 Jan  3 19:48 .terraform
-rw-r--r--   1 t_gadar  staff  3528 Jan  3 19:54 eks.tf
-rw-r--r--   1 t_gadar  staff  1123 Jan  3 19:54 network.tf
-rw-r--r--   1 t_gadar  staff    44 Jan  3 19:54 output.tf
-rw-r--r--   1 t_gadar  staff   512 Jan  3 19:54 portworx.tf
-rw-r--r--   1 t_gadar  staff  1237 Jan  3 19:54 provider.tf
-rw-r--r--   1 t_gadar  staff  1226 Jan  3 19:54 sg.tf
-rw-r--r--   1 t_gadar  staff   846 Jan  3 19:54 variable.tf
-rw-r--r--   1 t_gadar  staff   142 Jan  3 19:54 terraform.tfvars
drwxr-xr-x  11 t_gadar  staff   352 Jan  3 19:54 .
```

### Step 5. Configure terraform.tfvars [parameters]

Edit the file - vi or nano terraform.tfvars and replace the mandatory parameters to create the cluster

One of the parameter is AWS key-pair, that has to be created ahead in the same region where you want to create the cluster:

The below steps for creating the AWS Key Pair in-case you need to create:

- Login to AWS Console
- Navigate to EC2 Service
- Select/Click on "Key Pairs" on the left panel in Network & Security Section
- Click on "Create Key Pair" button and follow the steps
- Save the PEM key file that is downloaded automatically for future use to login to the nodes [****Note: If you loose this file, you will not be able to login to the nodes]

For additional information, you can check the following link here https://docs.aws.amazon.com/cli/latest/userguide/cli-services-ec2-keypairs.html


#### terraform.tfvars
```
region                       = "us-west-2"  // Zone where cluster will be deployed
number_of_nodes              = "4"          // # of nodes
ec2_instance_type            = "c5a.xlarge" // Valid EC2 machine name
purestorage_aws_keypair      = "purestorage-px-eks-kp" // AWS Keypair name (not the .pem filename)
project_name                 = "pxProject"  // Unique Project name
cluster_name                 = "px_demo"    // Unique Cluster Name
k8s_version                  = "1.21"       // Kubernetes Engine Version
px_operator_version          = "1.6.1"      // portworx operator version

px_cloud_storage_type        = "gp2"        // aws storage type (gp2 or gp3)
px_cloud_storage_size        = "30"         // portworx volume size per node
px_kvdb_device_storage_type  = "gp2"        // aws storage type (gp2 or gp3)
px_kvdb_device_storage_size  = "40"         // portworx KVDB volume size
px_storage_cluster_version   = "2.9.0"      // Px Storage Cluster Version
```

### Step 6. Validate and Execute

```
terraform init
terraform validate
terraform plan -var-file="terraform.tfvars" -out plan.out
terraform apply "plan.out"
```

This completes the creation of EKS cluster with Portworx, and the output of cluster name is generated.
Note: A new kube config file will be created at ~/.kube/config, and the existing kube config file will be backed up with date and time stamp.


### Destroy specific resource from terraform

Follow the below steps to apply portworx changes

```
export AWS_PROFILE=purestorage_aws_dev

For ex:
terraform destroy target null_resource.install_portworx -auto-approve

terraform plan -out "plan.out"
terraform apply "plan.out"
```

## Cleanup steps:

Step 1:
Configure Kubeconfig to the EKS cluster that you would like to clean up

```
        export KUBECONFIG="$PWD/kube-config-file"

        if the file does not existing run the following to create the kube config file

        aws eks --region <<aws region>> update-kubeconfig --name <<cluster-name>>

        Test: 
        kubectl get nodes //should return the nodes
```

Step 2: Delete any other namespaces that may have created on the cluster

Step 3: Export desired AWS account profile (Skip if already exported) and then run the terraform destroy command:

```
export AWS_PROFILE=purestorage_aws_dev
terraform destroy -auto-approve
```

### Destroy specific resource from terraform:

```
export AWS_PROFILE=purestorage_aws_dev
terraform destroy target null_resource.install_portworx -auto-approve
```


Note: the terraform destroy command needs to be executed from the same location as terraform apply command. 


## Additonal Notes:

### How to SSH into the nodes:

```
- Open your terminal and change directory with command cd, where you downloaded the pem file (from step 5)
- The private key (pem file) must be protected from read and write operations from any other users, SSH will not work in case it is open to read/write by other users
        Command:
                $ chmod 0400 .ssh/my_private_key.pem
- To connec to your ec2 instance/node, enter the following command
        Command:
                $ ssh -i /path/my-key-pair.pem my-instance-user-name@my-instance-IPv6-address
                $ ssh -i /path/my-key-pair.pem my-instance-user-name@my-instance-public-dns-name

``` 
Reference:
1. [Connecting to Linux instance using an SSH  client](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AccessingInstancesLinux.html)
2. [Troubleshoot Connecting to your Instances ](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/TroubleshootingInstancesConnecting.html)



## Using Docker image (Optional)


Build Docker Image using the below shell script;

```
./buildDockerImage.sh
```


Setup AWS credentials using the following environment variables:
```
export AWS_ACCESS_KEY_ID=$(aws --profile <aws_profile> configure get aws_access_key_id)
export AWS_SECRET_ACCESS_KEY=$(aws --profile <aws_profile> configure get aws_secret_access_key)
```

Execute the docker image using the below shell script:

``` 
./runDockerImage.sh
```

### Terraform Plan and Apply

Navigate to the folder that contains terraform files:

```
cd aws-<accountnumber>/<region>

The folder should have the following files:

~/purestorage/terraform-iac/terraform-live/aws-077119361744/us-west-1 master  :ll
total 80
-rw-r--r--  1 t_gadar  staff   235 Dec 15 11:57 main.tf
-rw-r--r--  1 t_gadar  staff  1123 Dec 15 11:57 network.tf
-rw-r--r--  1 t_gadar  staff    44 Dec 15 11:57 output.tf
-rw-r--r--  1 t_gadar  staff   359 Dec 15 11:57 portworx.tf
-rw-r--r--  1 t_gadar  staff  1237 Dec 15 11:57 provider.tf
-rw-r--r--  1 t_gadar  staff  1226 Dec 15 11:57 sg.tf
-rw-r--r--  1 t_gadar  staff   509 Dec 15 12:00 variable.tf
-rw-r--r--  1 t_gadar  staff  3467 Dec 15 12:03 eks.tf

```

Follow the Step 3 - 6 from non-docker steps

